input {
  file {
    path => "/logstash01/data/ingest_data/*"
    mode => "tail"
    type => "csv"
  }

  file {
    path => "/logstash01/data/ingest_data/*.log"
    type => "log"
  }
}

filter {
  if [type] == "csv" {
    # Configuración específica para archivos CSV
    csv {
      separator => ","
      autodetect_column_names => true
    }

  } else if [type] == "log" {
    # Configuración específica para archivos de registro (log)
    if [message] =~ "sequenceId" {
      # Synology
      grok {
        match => { "message" => '<%{POSINT:syslog_pri}>%{INT:version} %{TIMESTAMP_ISO8601:timestamp} %{HOSTNAME:hostname} %{DATA:syslog_program} - - (?:\[.+sequenceId="%{POSINT:message_id}"])? %{GREEDYDATA:log_message}' }
        add_field => [ "source", "%{hostname}" ]
      }
      syslog_pri { }
    } else if [message] =~ "GET /" {
      # Filtro para logs de IIS (ejemplo)
      grok {
        match => { "message" => "%{IP:client_ip} %{USER:username} %{HTTPDATE:timestamp} %{WORD:http_verb} %{URIPATH:uri_path} %{NOTSPACE:http_version} %{NUMBER:http_status} %{NUMBER:response_size}" }
        add_field => [ "[host][name]", "IIS" ]
        add_field => [ "log_message", "%{message}" ]
      }
    } else if [message] =~ "EventCode=" {
      # Filtro para logs del sistema operativo de Windows (ejemplo)
      grok {
        match => { "message" => "EventCode=%{NUMBER:event_code} %{GREEDYDATA:log_message}" }
        add_field => [ "[host][name]", "Windows" ]
      }
    } else {
      # Otros registros syslog
      grok {
        match => { "message" => "%{SYSLOGTIMESTAMP:timestamp} %{SYSLOGHOST:hostname} %{DATA:program}(?:\[%{POSINT:pid}\])?: %{GREEDYDATA:log_message}" }
        add_field => [ "source", "%{hostname}" ]
      }
      date {
        match => [ "timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
        target => "timestamp"
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["https://elasticsearch:9200"]
    user => "elastic"
    password => "WdeOwxFCeHsVkRjadasdsadtLJASDJADJ"
    ssl_enabled => true
    ssl_verification_mode => full
    ssl_certificate_authorities => '/usr/share/logstash/config/certs/ca/ca.crt'
    index => "logsstash-%{+YYYY.MM.dd}"
  }
}



TEST INGESTA CSV  Y LOGS:
=========================

Para crear un archivo de log (ejemplo.log):
cat << EOF > /ruta/local/ingest_data/ejemplo.log
2024-01-01 10:00:00 - Registro de ejemplo
2024-01-01 11:00:00 - Otro registro de ejemplo
EOF
Reemplaza /ruta/local/ con la ruta real de tu sistema local.

Para crear un archivo CSV (ejemplo.csv):
cat << EOF > /ruta/local/ingest_data/ejemplo.csv
Nombre,Edad,País
Juan,25,Argentina
María,30,Chile
EOF


[2024-01-19T14:32:04,972][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>8}
[2024-01-19T14:32:04,985][INFO ][logstash.outputs.elasticsearch][main] Not eligible for data streams because config contains one or more settings that are not compatible with data streams: {"index"=>"logsstash-%{+YYYY.MM.dd}"}
[2024-01-19T14:32:04,986][INFO ][logstash.outputs.elasticsearch][main] Data streams auto configuration (`data_stream => auto` or unset) resolved to `false`
[2024-01-19T14:32:04,989][WARN ][logstash.filters.grok    ][main] ECS v8 support is a preview of the unreleased ECS v8, and uses the v1 patterns. When Version 8 of the Elastic Common Schema becomes available, this plugin will need to be updated
[2024-01-19T14:32:05,014][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>8, :ecs_compatibility=>:v8}
[2024-01-19T14:32:05,158][INFO ][logstash.filters.csv     ][main] ECS compatibility is enabled but `target` option was not specified. This may cause fields to be set at the top-level of the event where they are likely to clash with the Elastic Common Schema. It is recommended to set the `target` option to avoid potential schema conflicts (if your data is ECS compliant or non-conflicting, feel free to ignore this message)
[2024-01-19T14:32:05,158][WARN ][logstash.filters.grok    ][main] ECS v8 support is a preview of the unreleased ECS v8, and uses the v1 patterns. When Version 8 of the Elastic Common Schema becomes available, this plugin will need to be updated
[2024-01-19T14:32:05,207][WARN ][logstash.filters.grok    ][main] ECS v8 support is a preview of the unreleased ECS v8, and uses the v1 patterns. When Version 8 of the Elastic Common Schema becomes available, this plugin will need to be updated
/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/jls-grok-0.11.5/lib/grok-pure.rb:127: warning: regular expression has ']' without escape
[2024-01-19T14:32:05,248][WARN ][logstash.filters.grok    ][main] ECS v8 support is a preview of the unreleased ECS v8, and uses the v1 patterns. When Version 8 of the Elastic Common Schema becomes available, this plugin will need to be updated
[2024-01-19T14:32:05,301][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>500, "pipeline.sources"=>["/usr/share/logstash/pipeline/logstash.conf"], :thread=>"#<Thread:0xd5ce5f3 /usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:134 run>"}
[2024-01-19T14:32:06,405][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>1.1}
[2024-01-19T14:32:06,421][INFO ][logstash.inputs.file     ][main] No sincedb_path set, generating one based on the "path" setting {:sincedb_path=>"/usr/share/logstash/data/plugins/inputs/file/.sincedb_bd21561d4f4ca60ed9d1fa3d5826a7a0", :path=>["/usr/share/logstash/data/ingest_data/*"]}
[2024-01-19T14:32:06,425][INFO ][logstash.inputs.file     ][main] No sincedb_path set, generating one based on the "path" setting {:sincedb_path=>"/usr/share/logstash/data/plugins/inputs/file/.sincedb_fd0b84ef6fe4f7d4df0995c7b173c4c0", :path=>["/usr/share/logstash/data/ingest_data/*.log"]}
[2024-01-19T14:32:06,427][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2024-01-19T14:32:06,441][INFO ][filewatch.observingtail  ][main][d551b559fd632f5de38dece31a06d7762ff28428a4f0523f64c809103f58d2ff] START, creating Discoverer, Watch with file and sincedb collections
[2024-01-19T14:32:06,449][INFO ][filewatch.observingtail  ][main][60a21a3487c3af7f1afd71234cbb771c458bf4fb67b9ae69d0fa723a164d7459] START, creating Discoverer, Watch with file and sincedb collections
[2024-01-19T14:32:06,447][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/manticore-0.9.1-java/lib/manticore/client.rb:284: warning: already initialized constant Manticore::Client::HttpPost
/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/manticore-0.9.1-java/lib/manticore/client.rb:534: warning: already initialized constant Manticore::Client::ByteArrayEntity

