input {
  file {
    path => "/logstash01/data/ingest_data/*"
    mode => "tail"
    type => "csv"
  }

  file {
    path => "/logstash01/data/ingest_data/*.log"
    type => "log"
  }
}

filter {
  if [type] == "csv" {
    # Configuración específica para archivos CSV
    csv {
      separator => ","
      autodetect_column_names => true
    }

  } else if [type] == "log" {
    # Configuración específica para archivos de registro (log)
    if [message] =~ "sequenceId" {
      # Synology
      grok {
        match => { "message" => '<%{POSINT:syslog_pri}>%{INT:version} %{TIMESTAMP_ISO8601:timestamp} %{HOSTNAME:hostname} %{DATA:syslog_program} - - (?:\[.+sequenceId="%{POSINT:message_id}"])? %{GREEDYDATA:log_message}' }
        add_field => [ "source", "%{hostname}" ]
      }
      syslog_pri { }
    } else if [message] =~ "GET /" {
      # Filtro para logs de IIS (ejemplo)
      grok {
        match => { "message" => "%{IP:client_ip} %{USER:username} %{HTTPDATE:timestamp} %{WORD:http_verb} %{URIPATH:uri_path} %{NOTSPACE:http_version} %{NUMBER:http_status} %{NUMBER:response_size}" }
        add_field => [ "[host][name]", "IIS" ]
        add_field => [ "log_message", "%{message}" ]
      }
    } else if [message] =~ "EventCode=" {
      # Filtro para logs del sistema operativo de Windows (ejemplo)
      grok {
        match => { "message" => "EventCode=%{NUMBER:event_code} %{GREEDYDATA:log_message}" }
        add_field => [ "[host][name]", "Windows" ]
      }
    } else {
      # Otros registros syslog
      grok {
        match => { "message" => "%{SYSLOGTIMESTAMP:timestamp} %{SYSLOGHOST:hostname} %{DATA:program}(?:\[%{POSINT:pid}\])?: %{GREEDYDATA:log_message}" }
        add_field => [ "source", "%{hostname}" ]
      }
      date {
        match => [ "timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
        target => "timestamp"
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["https://elasticsearch:9200"]
    user => "elastic"
    password => "WdeOwxFCeHsVkRjadasdsadtLJASDJADJ"
    ssl_enabled => true
    ssl_verification_mode => full
    ssl_certificate_authorities => '/usr/share/logstash/config/certs/ca/ca.crt'
    index => "logsstash-%{+YYYY.MM.dd}"
  }
}



TEST INGESTA CSV  Y LOGS:
=========================

Para crear un archivo de log (ejemplo.log):
cat << EOF > /ruta/local/ingest_data/ejemplo.log
2024-01-01 10:00:00 - Registro de ejemplo
2024-01-01 11:00:00 - Otro registro de ejemplo
EOF
Reemplaza /ruta/local/ con la ruta real de tu sistema local.

Para crear un archivo CSV (ejemplo.csv):
cat << EOF > /ruta/local/ingest_data/ejemplo.csv
Nombre,Edad,País
Juan,25,Argentina
María,30,Chile
EOF


[2024-01-19T14:32:04,972][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>8}
[2024-01-19T14:32:04,985][INFO ][logstash.outputs.elasticsearch][main] Not eligible for data streams because config contains one or more settings that are not compatible with data streams: {"index"=>"logsstash-%{+YYYY.MM.dd}"}
[2024-01-19T14:32:04,986][INFO ][logstash.outputs.elasticsearch][main] Data streams auto configuration (`data_stream => auto` or unset) resolved to `false`
[2024-01-19T14:32:04,989][WARN ][logstash.filters.grok    ][main] ECS v8 support is a preview of the unreleased ECS v8, and uses the v1 patterns. When Version 8 of the Elastic Common Schema becomes available, this plugin will need to be updated
[2024-01-19T14:32:05,014][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>8, :ecs_compatibility=>:v8}
[2024-01-19T14:32:05,158][INFO ][logstash.filters.csv     ][main] ECS compatibility is enabled but `target` option was not specified. This may cause fields to be set at the top-level of the event where they are likely to clash with the Elastic Common Schema. It is recommended to set the `target` option to avoid potential schema conflicts (if your data is ECS compliant or non-conflicting, feel free to ignore this message)
[2024-01-19T14:32:05,158][WARN ][logstash.filters.grok    ][main] ECS v8 support is a preview of the unreleased ECS v8, and uses the v1 patterns. When Version 8 of the Elastic Common Schema becomes available, this plugin will need to be updated
[2024-01-19T14:32:05,207][WARN ][logstash.filters.grok    ][main] ECS v8 support is a preview of the unreleased ECS v8, and uses the v1 patterns. When Version 8 of the Elastic Common Schema becomes available, this plugin will need to be updated
/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/jls-grok-0.11.5/lib/grok-pure.rb:127: warning: regular expression has ']' without escape
[2024-01-19T14:32:05,248][WARN ][logstash.filters.grok    ][main] ECS v8 support is a preview of the unreleased ECS v8, and uses the v1 patterns. When Version 8 of the Elastic Common Schema becomes available, this plugin will need to be updated
[2024-01-19T14:32:05,301][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>500, "pipeline.sources"=>["/usr/share/logstash/pipeline/logstash.conf"], :thread=>"#<Thread:0xd5ce5f3 /usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:134 run>"}
[2024-01-19T14:32:06,405][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>1.1}
[2024-01-19T14:32:06,421][INFO ][logstash.inputs.file     ][main] No sincedb_path set, generating one based on the "path" setting {:sincedb_path=>"/usr/share/logstash/data/plugins/inputs/file/.sincedb_bd21561d4f4ca60ed9d1fa3d5826a7a0", :path=>["/usr/share/logstash/data/ingest_data/*"]}
[2024-01-19T14:32:06,425][INFO ][logstash.inputs.file     ][main] No sincedb_path set, generating one based on the "path" setting {:sincedb_path=>"/usr/share/logstash/data/plugins/inputs/file/.sincedb_fd0b84ef6fe4f7d4df0995c7b173c4c0", :path=>["/usr/share/logstash/data/ingest_data/*.log"]}
[2024-01-19T14:32:06,427][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2024-01-19T14:32:06,441][INFO ][filewatch.observingtail  ][main][d551b559fd632f5de38dece31a06d7762ff28428a4f0523f64c809103f58d2ff] START, creating Discoverer, Watch with file and sincedb collections
[2024-01-19T14:32:06,449][INFO ][filewatch.observingtail  ][main][60a21a3487c3af7f1afd71234cbb771c458bf4fb67b9ae69d0fa723a164d7459] START, creating Discoverer, Watch with file and sincedb collections
[2024-01-19T14:32:06,447][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/manticore-0.9.1-java/lib/manticore/client.rb:284: warning: already initialized constant Manticore::Client::HttpPost
/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/manticore-0.9.1-java/lib/manticore/client.rb:534: warning: already initialized constant Manticore::Client::ByteArrayEntity



input {
  beats {
    port => "5044"
  }
}

filter {
  if [type] == "csv" {
    csv {
      separator => ","
    }
  } else if [type] == "log" {
    if [message] =~ "sequenceId" {
      grok {
        match => { "message" => '<%{POSINT:syslog_pri}>%{INT:version} %{TIMESTAMP_ISO8601:timestamp} %{HOSTNAME:hostname} %{DATA:syslog_program} - - (?:\[.+sequenceId="%{POSINT:message_id}"])? %{GREEDYDATA:log_message}' }
        add_field => [ "source", "%{hostname}" ]
      }
      syslog_pri { }
    } else if [message] =~ "GET /" {
      grok {
        match => { "message" => "%{IP:client_ip} %{USER:username} %{HTTPDATE:timestamp} %{WORD:http_verb} %{URIPATH:uri_path} %{NOTSPACE:http_version} %{NUMBER:http_status} %{NUMBER:response_size}" }
        add_field => [ "[host][name]", "IIS" ]
        add_field => [ "log_message", "%{message}" ]
      }
    } else if [message] =~ "EventCode=" {
      grok {
        match => { "message" => "EventCode=%{NUMBER:event_code} %{GREEDYDATA:log_message}" }
        add_field => [ "[host][name]", "Windows" ]
      }
    } else {
      grok {
        match => { "message" => "%{SYSLOGTIMESTAMP:timestamp} %{SYSLOGHOST:hostname} %{DATA:program}(?:\[%{POSINT:pid}\])?: %{GREEDYDATA:log_message}" }
        add_field => [ "source", "%{hostname}" ]
      }
      date {
        match => [ "timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
        target => "timestamp"
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["https://elasticsearch:9200"]
    user => "elastic"
    password => "xxxxxxxxxxxx"
    ssl_enabled => true
    ssl_verification_mode => full
    ssl_certificate_authorities => '/usr/share/logstash/config/certs/ca/ca.crt'
    index => "logsstash-%{+YYYY.MM.dd}"
    index.number_of_replicas => 0
  }
}


[root@dkr-elk-hc01 ~]# podman logs logstash01
2024/01/19 18:09:41 Setting 'xpack.monitoring.enabled' from environment.
Using bundled JDK: /usr/share/logstash/jdk
/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/concurrent-ruby-1.1.9/lib/concurrent-ruby/concurrent/executor/java_thread_pool_executor.rb:13: warning: method redefined; discarding old to_int
/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/concurrent-ruby-1.1.9/lib/concurrent-ruby/concurrent/executor/java_thread_pool_executor.rb:13: warning: method redefined; discarding old to_f
Sending Logstash logs to /usr/share/logstash/logs which is now configured via log4j2.properties
[2024-01-19T18:09:55,880][INFO ][logstash.runner          ] Log4j configuration path used is: /usr/share/logstash/config/log4j2.properties
[2024-01-19T18:09:55,888][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"8.11.3", "jruby.version"=>"jruby 9.4.5.0 (3.1.4) 2023-11-02 1abae2700f OpenJDK 64-Bit Server VM 17.0.9+9 on 17.0.9+9 +indy +jit [x86_64-linux]"}
[2024-01-19T18:09:55,891][INFO ][logstash.runner          ] JVM bootstrap flags: [-Xms1g, -Xmx1g, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djruby.compile.invokedynamic=true, -XX:+HeapDumpOnOutOfMemoryError, -Djava.security.egd=file:/dev/urandom, -Dlog4j2.isThreadContextMapInheritable=true, -Dls.cgroup.cpuacct.path.override=/, -Dls.cgroup.cpu.path.override=/, -Djruby.regexp.interruptible=true, -Djdk.io.File.enableADS=true, --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED, --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED, --add-opens=java.base/java.security=ALL-UNNAMED, --add-opens=java.base/java.io=ALL-UNNAMED, --add-opens=java.base/java.nio.channels=ALL-UNNAMED, --add-opens=java.base/sun.nio.ch=ALL-UNNAMED, --add-opens=java.management/sun.management=ALL-UNNAMED]
[2024-01-19T18:09:56,744][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600, :ssl_enabled=>false}
[2024-01-19T18:09:57,287][ERROR][logstash.agent           ] Failed to execute action {:action=>LogStash::PipelineAction::Create/pipeline_id:main, :exception=>"LogStash::ConfigurationError", :message=>"Expected one of [A-Za-z0-9_-], [ \\t\\r\\n], \"#\", \"=>\" at line 70, column 10 (byte 2268) after output {\n  elasticsearch {\n    hosts => [\"https://elasticsearch:9200\"]\n    user => \"elastic\"\n    password => \"WdeOwxFCeHsVkRjadasdsadtLJASDJADJ\"\n    ssl_enabled => true\n    ssl_verification_mode => full\n    ssl_certificate_authorities => '/usr/share/logstash/config/certs/ca/ca.crt'\n    index => \"logsstash-%{+YYYY.MM.dd}\"\n    index", :backtrace=>["/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:32:in `compile_imperative'", "org/logstash/execution/AbstractPipelineExt.java:239:in `initialize'", "org/logstash/execution/AbstractPipelineExt.java:173:in `initialize'", "/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:48:in `initialize'", "org/jruby/RubyClass.java:931:in `new'", "/usr/share/logstash/logstash-core/lib/logstash/pipeline_action/create.rb:49:in `execute'", "/usr/share/logstash/logstash-core/lib/logstash/agent.rb:386:in `block in converge_state'"]}
[2024-01-19T18:09:57,305][INFO ][logstash.runner          ] Logstash shut down.
[2024-01-19T18:09:57,312][FATAL][org.logstash.Logstash    ] Logstash stopped processing because of an error: (SystemExit) exit
org.jruby.exceptions.SystemExit: (SystemExit) exit
        at org.jruby.RubyKernel.exit(org/jruby/RubyKernel.java:808) ~[jruby.jar:?]
        at org.jruby.RubyKernel.exit(org/jruby/RubyKernel.java:767) ~[jruby.jar:?]
        at usr.share.logstash.lib.bootstrap.environment.<main>(/usr/share/logstash/lib/bootstrap/environment.rb:90) ~[?:?]
